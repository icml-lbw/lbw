<!DOCTYPE html>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">
    google.load("jquery", "1.3.2");
</script>

<html>

<head>
    <title>Learning by Watching: Physical Imitation of Manipulation Skills from Human Videos</title>
    <meta property="og:title"
        content="Learning by Watching: Physical Imitation of Manipulation Skills from Human Videos" />
    <link rel="stylesheet" href="style.css">
</head>

<body>
    <br>
    <center>
        <span style="font-size:42px">Learning by Watching: Physical Imitation of Manipulation Skills from Human Videos</span>
        <br><br>
        <span style="font-size:20px">Preprint. Under review.</span>
        <br><br>
        <table align=center>
            <tr>
                <span style="font-size:20px"><a href="">Authors Anonymous.</a></span> &nbsp;
            </tr>
        </table>
    </center>
        
    <hr>


    <h1 align=center>Abstract</h1>
    <span style="font-size:20px" align=left font-family: arial>
    Learning from visual data opens the potential to accrue a large range of manipulation behaviors by 
        leveraging human demonstrations without specifying each of them mathematically, 
        but rather through natural task specification. In this paper, we present Learning by Watching (LbW), 
        an algorithmic framework for policy learning through imitation from a single video specifying the task. 
        The key insights of our method are two-fold.First, since the human arms may not have the same morphology as robot arms, 
        our framework learns unsupervised human to robot translation to overcome the morphology mismatch issue. Second, to capture 
        the details in salient regions that are crucial for learning state representations, our model performs unsupervised keypoint 
        detection on the translated robot videos. The detected keypoints form a structured representation that contains semantically 
        meaningful information and can be used directly for computing reward and policy learning. We evaluate the effectiveness of our LbW 
        framework on five robot manipulation tasks, including reaching, pushing, sliding, coffee making, and drawer closing. Extensive 
        experimental evaluations demonstrate that our method performs favorably against the state-of-the-art approaches.
    </span>
    <br><br>
    <img src="data/goal.gif" alt="LbW Overview Image" style="width:50%;margin: auto;display: block;">
    <br>
    <span style="font-size:20px" align=left font-family: arial>
    The resulting keypoint-based representations provide semantically meaningful information that can be directly used for 
        reward computing and policy learning. We evaluate the effectiveness of our approach on five robot manipulation tasks, 
        including reaching, pushing, sliding, coffee making, and drawer closing. Detailed experimental evalu- ations demonstrate 
        that our method performs favorably against previous approaches.
    </span>
    <br><br>
    
    <hr>
    
    <h1 align=center>Motivation</h1>
    <span style="font-size:20px" align=left font-family: arial>
    In robotic imitation learning, collecting expert demonstrations remains expensive and challenging as it assumes access to both 
        observations and actions. We aim to relax this expert supervision to human videos alone.
    </span>
    <br><br>
    <span style="font-size:20px" align=left font-family: arial>
    To bridge the human-robot domain gap, one way is to translate the human videos to the robot domain using a generative 
        modeling approach. However, translation is generally imperfect.
    </span>    
    <br><br>
    <span style="font-size:20px" align=left font-family: arial>
    Our key insight: explicitly exploiting the kinematics and motion information embedded in the video to learn structured 
        representations via translation and unsupervised keypoint detection.
    </span>
    <br><br>
    
    <hr>
    
    

</html>
